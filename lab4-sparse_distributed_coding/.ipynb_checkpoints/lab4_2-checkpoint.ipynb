{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../style/img/vs265header.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lab 4 - Sparse, Distributed Representations</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Sparse Coding of Natural Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.plotFunctions as pf\n",
    "import utils.helperFunctions as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of lab 4 we generated our own data, so we knew exactly what the underlying generators of the data were. Here, we are going to attempt to learn the generators of a richer input ensemble: images of the natural world. We can't do this with the Foldiak sparse coding model because it only works with binary signals. Instead, we are going to learn a sparse coding dictionary as was originally described in Olshausen & Field's 1996 and 1997 papers. The algorithm we are going to use to compute sparse codes is Rozell's Locally Competitive Algorithm (LCA), as described in his 2008 paper. LCA is explained in detail in a course handout, read that before going forward!\n",
    "\n",
    "The training data are obtained by extracting small image patches from whitened natural scenes, which one can think of as an idealization of the input provided by the LGN, as we discussed in class when learning about whitening transforms.\n",
    "\n",
    "Run the algorithm using 64 output neurons on 8x8 pixel image patches. To do this you must:\n",
    "\n",
    "* Fill in the LCA equations in the `lcaSparsify` function.\n",
    "\n",
    "* Fill in the $\\phi$ update learning rule in the `updatePhi` function.\n",
    "\n",
    "Verify that you can reconstruct an image from the set of learned features and comment on what was learned as well as the parameters used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below sets the parametrs for the sparse coding model. `numInputs` has been set to 64 to learn 8x8 pixel basis functions. This is not a hard constraint, so feel free to try out different patch sizes. You should also explore the effects of changing the `sparsityTradeoff` (i.e. $\\lambda$) parameter. The LCA model has two key additional parameters: how long to perform inference is set by `numSteps` and the membrane integration time constant is set by the variable `tau` (i.e. $\\tau$). A lower value for $\\tau$ causes the LCA model to perform a more coarse estimate of the true dynamics and therefore come to an approximate solution in fewer steps. FInally, the `numOutputs` parameter establishes the overcompleteness of the model. What is the effect of changing the `numOutputs` and `sparsityTradeoff` parameters?\n",
    "\n",
    " <b>Notes:</b> The LCA model takes longer than Foldiak's model to run. Make sure you always set `numInputs` to a value that has an even square root. Finally, `numOutputs` should be assigned to a multiple of `numInputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General sparse coding parameters\n",
    "numTrials = 1200 # Number of weight learning steps\n",
    "numInputs = 64 # Number of input pixels, needs to have an even square root\n",
    "numOutputs = 64 # Number of sparse coding neurons\n",
    "sparsityTradeoff = # YOUR CODE HERE # Lambda parameter that determines how sparse the model will be\n",
    "batchSize = 500 # How many image patches to include in batch\n",
    "eta = 0.08 # Learning rate\n",
    "\n",
    "# LCA specific parameters\n",
    "tau = 50 # LCA update time constant\n",
    "numSteps = 20 # Number of iterations to run LCA\n",
    "\n",
    "# Plot display parameters\n",
    "displayInterval = 50 # How often to update display plots during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert numInputs%np.sqrt(numInputs) == 0, (\n",
    "    \"numInputs must have an even square root.\")\n",
    "\n",
    "# Load images and view them\n",
    "dataset = np.load(\"data/IMAGES.npy\")\n",
    "[pixelsCols, pixelsRows, numImages] = dataset.shape\n",
    "numPixels = pixelsCols * pixelsRows\n",
    "dataset = dataset.reshape(numPixels, numImages)\n",
    "dataset /= np.sqrt(np.var(dataset)) # We want the dataset to have variance=1\n",
    "\n",
    "# Note: Here you can index any image, or just delete the [:,0] part and plot all images\n",
    "pf.plotDataTiled(dataset[:,0], \"Example Image Dataset\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lcaSparsify(data, phi, tau, sparsityTradeoff, numSteps):\n",
    "    \"\"\"\n",
    "    Compute sparse code of input data using the LCA\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray of dimensions (numInputs, batchSize) holding a batch of image patches\n",
    "    phi : np.ndarray of dimensions (numInputs, numOutputs) holding sparse coding dictionary\n",
    "    tau : float for setting time constant for LCA differential equation\n",
    "    sparsityTradeoff : float indicating Sparse Coding lambda value (also LCA neuron threshold)\n",
    "    numSteps: int indicating number of inference steps for the LCA model\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : np.ndarray of dimensions (numOutputs, batchSize) holding thresholded potentials\n",
    "    \"\"\"\n",
    "    b = # YOUR CODE HERE # Driving input\n",
    "    gramian = phi.T @ phi - np.identity(int(phi.shape[1])) # Explaining away matrix\n",
    "    u = np.zeros_like(b) # Initialize membrane potentials to 0\n",
    "    for step in range(numSteps):\n",
    "        a = hf.lcaThreshold(u, sparsityTradeoff) # Activity vector contains thresholded membrane potentials\n",
    "        du = # YOUR CODE HERE # LCA dynamics define membrane update\n",
    "        u = u + (1.0/tau) * du # Update membrane potentials using time constant\n",
    "    return hf.lcaThreshold(u, sparsityTradeoff)\n",
    "\n",
    "def lcaLearn(phi, patchBatch, sparseCode, learningRate):\n",
    "    patchBatchRecon = # YOUR CODE HERE # Reconstruct input using the inferred sparse code\n",
    "    reconError = # YOUR CODE HERE # Error between the input and reconstruction\n",
    "    dPhi = # YOUR CODE HERE # Weight update rule (dE/dPhi)\n",
    "    phi = phi + learningRate * dPhi # Scale weight update by learning rate\n",
    "    return (phi, reconError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize some empty arrays to hold network summary statistics\n",
    "percNonzero = np.zeros(numTrials)\n",
    "energy = np.zeros(numTrials)\n",
    "reconQuality = np.zeros(numTrials)\n",
    "\n",
    "# Initialize Phi weight matrix with random values\n",
    "phi = hf.l2Norm(np.random.randn(numInputs, numOutputs))\n",
    "\n",
    "# Do sparse coding with LCA\n",
    "prevFig = pf.plotDataTiled(phi, \"Dictionary at time step 0\", None)\n",
    "for trial in range(numTrials):\n",
    "    # Make batch of random image patches\n",
    "    patchBatch = np.zeros((numInputs, batchSize))\n",
    "    for batchNum in range(batchSize):\n",
    "        patchBatch[:, batchNum] = hf.getRandomPatch(dataset, int(np.sqrt(numInputs)))\n",
    "\n",
    "    # Compute sparse code for batch of image patches\n",
    "    sparseCode = lcaSparsify(patchBatch, phi, tau, sparsityTradeoff, numSteps)\n",
    "    \n",
    "    # Update weights using inferred sparse code\n",
    "    learningRate = eta / batchSize\n",
    "    (phi, reconError) = lcaLearn(phi, patchBatch, sparseCode, learningRate)\n",
    "    \n",
    "    # Renormalize phi matrix\n",
    "    phi = hf.l2Norm(phi)\n",
    "    \n",
    "    # Record some stats for plotting\n",
    "    (percNonzero[trial], energy[trial], reconQuality[trial]) = hf.computePlotStats(sparseCode, reconError, sparsityTradeoff)\n",
    "\n",
    "    # Update dictionary plot\n",
    "    if trial % displayInterval == 0:\n",
    "        prevFig = pf.plotDataTiled(phi, \"Dictionary at time step \"+str(trial), prevFig)\n",
    "    \n",
    "# Plot learned dictionary\n",
    "prevFig = pf.plotDataTiled(phi, \"Dictionary at time step \"+str(trial), prevFig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot learning summary statistics\n",
    "dataList = [energy, percNonzero, reconQuality]\n",
    "labelList = [\"Energy\", \"% Non-Zero Activations\", \"Recon Quality pSNR dB\"]\n",
    "title = \"Summary Statistics for LCA Sparse Coding\"\n",
    "pf.makeSubplots(dataList, labelList, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reconstruct an image\n",
    "image = dataset[:,0]\n",
    "imgPixels = image.size\n",
    "numPatches = int(imgPixels/numInputs) # must divide evenly\n",
    "patchBatch = image.reshape(numPatches, numInputs).T\n",
    "sparseCode = lcaSparsify(patchBatch, phi, tau, sparsityTradeoff, numSteps)\n",
    "reconBatch = phi @ sparseCode\n",
    "reconImage = reconBatch.T.reshape(imgPixels)\n",
    "imgAndRecon = np.vstack((image, reconImage)).T\n",
    "pf.plotDataTiled(imgAndRecon, \"Image and Corresponding Reconstruction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>YOUR ANSWER HERE:</b> What is the effect of changing the sparsity tradeoff parameter, $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try increasing the size of the network to 128 (or more) output neurons and decreasing the size to 32 (or less) output neurons. How do the learned features change as you modify the degree of overcompleteness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>YOUR ANSWER HERE:</b> What is the effect modifying the number of output neurons?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
